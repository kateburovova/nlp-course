{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"openai-gpt\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def generate_text(input_text: str, tokens_to_generate: int):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "text = \"The Hamilton is a work of\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "text_encoded = torch.tensor([tokenizer.encode(text)], dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(text_encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "the hamilton is a work of art. \" \n",
      " \" i'm sure it is. \" \n",
      " \" i'm sure it is. \" \n",
      " \" i'm sure it is. \" \n",
      " \" i'm sure it is. \" \n",
      " \" i'm sure it is. \" \n",
      " \" i'm sure it is. \" \n",
      " \" i'm sure it is. \" \n",
      " \" i'm sure it is. \" \n",
      " \" i'm sure it is. \" \n",
      " \" i'm sure it is. \" \n",
      " \" i'm sure it is\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(generate_text(text, 100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def gen_topK_text(input_text: str, tokens_to_generate: int, k: int):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            \n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "            next_token_id = torch.LongTensor(random.choices(next_token_logits.topk(k, dim=-1).indices.tolist()[0], torch.nn.functional.softmax(next_token_logits).topk(k, dim=-1).values.tolist()[0])).unsqueeze(-1)            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/macbook/PycharmProjects/nlp-course/venv/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "the hamilton is a work of art... \" \n",
      " \" i know, \" replied janet. \n",
      " \" you must be impressed. \" \n",
      " \" i am. it's a wonderful place and i can't wait to see the others, \" admitted janet. \n",
      " \" i hope they aren't too tired from yesterday's activities... \" \n",
      " janet shook her head. \" i hope not! and if the others didn't make it, i think they will be more interested in what i've learned tomorrow. \" \n",
      " chapter 14 \n",
      " a short\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gen_topK_text(text, 100, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/macbook/PycharmProjects/nlp-course/venv/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "the hamilton is a work of art. \" \n",
      " \" it's a beautiful place, \" i agreed. \n",
      " \" i've been here before. \" she smiled at me and i felt a pang of jealousy. i wasn't sure why, but i wanted to see her again, to be near her, even if she hadn't been there before. \n",
      " \" i'm glad you're here, \" i said softly, my eyes on the painting of the two of us. \n",
      " \" me too. \" \n",
      " \" i'm glad\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gen_topK_text(text, 100, 3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def gen_topK_text_penalized(input_text: str, tokens_to_generate: int, k: int, penalty: float):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            \n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "            \n",
    "            #applying penalty (>1 to cut repetition) to logits that are already in text_generated\n",
    "            #we add (penalty-1)% to logits < 0 and substract (penalty-1)% if logits > 0\n",
    "\n",
    "            if penalty != 1:\n",
    "              for i in range(len(next_token_logits)):\n",
    "                if i in text_generated[0]:\n",
    "                  if next_token_logits[0][i]>=0:\n",
    "                    next_token_logits[0][i] /= penalty  \n",
    "                  else:\n",
    "                    next_token_logits[0][i] *= penalty\n",
    "\n",
    "            next_token_id = torch.LongTensor(random.choices(next_token_logits.topk(k, dim=-1).indices.tolist()[0], torch.nn.functional.softmax(next_token_logits).topk(k, dim=-1).values.tolist()[0])).unsqueeze(-1)   \n",
    "            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/macbook/PycharmProjects/nlp-course/venv/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "the hamilton is a work of art, \" he said. \n",
      " \" i know, \" i said, \" but i'm not a fan. \" \n",
      " \" i know. \" \n",
      " \" you're a good guy, \" i said, \" but i'm not a fan. i'm not a fan of anything. \" \n",
      " \" i know, \" he said again. \" but i'm a fan of you. \" \n",
      " \" i'm not a fan of you, \" i repeated. \n",
      " \" i know. \" \n",
      " \" you don't know. \" \n",
      " \" i know, \" he agreed. \n",
      " i sighed. \" okay, then, i'm not a fan of you. \" \n",
      " \" good, \" he said. \" because i'm going to kiss you. \" \n",
      " i was still breathing hard. i couldn't help but notice that he hadn't taken his eyes off me, and i wasn't sure whether that was because he wanted to kiss or because of my reaction. \n",
      " \" you're a good\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gen_topK_text_penalized(text, 200, 3, 1.7))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gen_topK_text_penalized(input_text: str, tokens_to_generate: int, k: int, penalty: float):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            \n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "            \n",
    "            #applying penalty (>1 to cut repetition) to logits that are already in text_generated\n",
    "            #we add (penalty-1)% to logits < 0 and substract (penalty-1)% if logits > 0\n",
    "\n",
    "            if penalty != 1:\n",
    "              for i in range(len(next_token_logits)):\n",
    "                if i in text_generated[0]:\n",
    "                  if next_token_logits[0][i]>=0:\n",
    "                    next_token_logits[0][i] /= penalty  \n",
    "                  else:\n",
    "                    next_token_logits[0][i] *= penalty\n",
    "\n",
    "            #next_token_id = torch.LongTensor(random.choices(next_token_logits.topk(k, dim=-1).indices.tolist()[0], torch.nn.functional.softmax(next_token_logits).topk(k, dim=-1).values.tolist()[0])).unsqueeze(-1)   \n",
    "            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}