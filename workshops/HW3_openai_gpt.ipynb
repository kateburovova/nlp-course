{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"openai-gpt\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def generate_text(input_text: str, tokens_to_generate: int):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "text = \"My favourite movie is\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "text_encoded = torch.tensor([tokenizer.encode(text)], dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(text_encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "my favourite movie is the one where the guy is in love with the girl. \" \n",
      " \" i don't think that's a good idea. \" \n",
      " \" why not? \" \n",
      " \" because it's not a good idea. \" \n",
      " \" why not? \" \n",
      " \" because it's not a good idea. \" \n",
      " \" why not? \" \n",
      " \" because it's not a good idea. \" \n",
      " \" why not? \" \n",
      " \" because it's not a good idea. \" \n",
      " \" why not? \"\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(generate_text(text, 100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def gen_topK_text(input_text: str, tokens_to_generate: int, k: int):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            \n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "            next_token_id = torch.LongTensor(random.choices(next_token_logits.topk(k, dim=-1).indices.tolist()[0], torch.nn.functional.softmax(next_token_logits).topk(k, dim=-1).values.tolist()[0])).unsqueeze(-1)            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/macbook/PycharmProjects/nlp-course/venv/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "my favourite movie is about a guy who goes into a house with two girls at the end of the hall. the girls aren't as cute as they used to be, and i don't want to be the first person they see when they come out. \" she laughs. \" you 'd better get your butt over to that house. \" \n",
      " \" i can't believe you just said that, \" i mumble, trying not to laugh. \n",
      " \" come on, you 'll be glad you did, \" she says,\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gen_topK_text(text, 100, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/macbook/PycharmProjects/nlp-course/venv/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "my favourite movie is the one with the two of us. \" he smiled at her. \" i think you're going to like it, too. \" \n",
      " \" what do you mean? \" \n",
      " \" it's the one with the two of us. i'm going to make love to you in the middle of the night and we're going to be late for our appointment with dr. demarco. \" \n",
      " \" you're going to make love to me? \" she repeated, her heart racing. \" you're going to\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gen_topK_text(text, 100, 3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def gen_topK_text_penalized(input_text: str, tokens_to_generate: int, k: int, penalty: float):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            \n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "            \n",
    "            #applying penalty (>1 to cut repetition) to logits that are already in text_generated\n",
    "            #we add (penalty-1)% to logits < 0 and substract (penalty-1)% if logits > 0\n",
    "\n",
    "            if penalty != 1:\n",
    "              for i in range(len(next_token_logits)):\n",
    "                if i in text_generated[0]:\n",
    "                  if next_token_logits[0][i]>=0:\n",
    "                    next_token_logits[0][i] *= (1-penalty)  \n",
    "                  else:\n",
    "                    next_token_logits[0][i] *= (1+penalty)\n",
    "                            \n",
    "                            \n",
    "\n",
    "            next_token_id = torch.LongTensor(random.choices(next_token_logits.topk(k, dim=-1).indices.tolist()[0], torch.nn.functional.softmax(next_token_logits).topk(k, dim=-1).values.tolist()[0])).unsqueeze(-1)   \n",
    "            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/macbook/PycharmProjects/nlp-course/venv/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "my favourite movie is the one where you get a kiss. \" \n",
      " \" i'm sorry. i didn't know. \" \n",
      " \" i'm not. i'm just saying, you're a good guy. \" \n",
      " \" thanks. \" i smiled at him, feeling a little bit better. \n",
      " \" so, you want to come over and watch the movie? \" \n",
      " \" yeah, i guess so. \" \n",
      " \" cool. \" \n",
      " i sat down next to him on the sofa, my legs stretched out in front of me, and he pulled me into him, wrapping his arm around my waist. \" i've been meaning to ask you something. \" \n",
      " \" what? \" \n",
      " \" how did you get to be a vampire? \" \n",
      " \" i don't know. \" \n",
      " \" you're a vampire? \" \n",
      " \" yes. \" \n",
      " \" how? \" \n",
      " \" i'm not sure. i just know i was born. \" \n",
      " i laughed. \" you're kidding, right? \"\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gen_topK_text_penalized(text, 200, 2, 10))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def gen_topP_text_penalized(input_text: str, tokens_to_generate: int, p: float, penalty: float):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            \n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "            \n",
    "            #applying penalty (>1 to cut repetition) to logits that are already in text_generated\n",
    "            #we add (penalty-1)% to logits < 0 and substract (penalty-1)% if logits > 0\n",
    "\n",
    "            if penalty != 1:\n",
    "              for i in range(len(next_token_logits)):\n",
    "                if i in text_generated[0]:\n",
    "                  if next_token_logits[0][i]>=0:\n",
    "                    next_token_logits[0][i] *= (1-penalty)  \n",
    "                  else:\n",
    "                    next_token_logits[0][i] *= (1+penalty)\n",
    " \n",
    "            \n",
    "            cum_sum_prob = 0\n",
    "            chosen_logits_indices = []\n",
    "            sorted_softmax_values, indices = torch.sort(torch.nn.functional.softmax(next_token_logits), descending=True)\n",
    "            \n",
    "            for i in range(len(sorted_softmax_values[0])):\n",
    "              if cum_sum_prob <= p:\n",
    "                cum_sum_prob += sorted_softmax_values[0][i]\n",
    "                chosen_logits_indices.append(indices[0][i].tolist())\n",
    "              else:\n",
    "                break\n",
    "            \n",
    "            next_token_id = torch.LongTensor(random.choices(chosen_logits_indices)).unsqueeze(-1)\n",
    "            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/macbook/PycharmProjects/nlp-course/venv/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "my favourite movie is haunted, \" miranda informed me as she gave me a sly grin. \n",
      " i reached for the soda she 'd handed me. \" yes, \" i replied. \n",
      " \" it's not supposed to be on until six, \" she replied. \n",
      " \" you should have been able to turn it on. i 'd hate to ruin it, \" i told her. \n",
      " miranda turned to look at me. \" yeah, but i had no idea it would make me fall asleep, \" she teased. \n",
      " \" sorry. \" \n",
      " miranda rolled her eyes and walked back over to the chair she 'd vacated. i stood there a moment and thought about calling rush and leaving. it would be rude not to call him. he 'd probably tell me to fuck off and leave him be. \n",
      " i turned to leave when a loud banging at the door caught my attention. i headed over to see who it was and let rush's muffled voice drift out. i stood in the doorway and watched him for a moment before\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gen_topP_text_penalized(text, 200, 0.7, 5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def gen_topP_penalized_w_temp(input_text: str, tokens_to_generate: int, p: float, penalty: float, temperature: int =1):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            \n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "            \n",
    "            #applying penalty (>1 to cut repetition) to logits that are already in text_generated\n",
    "            #we add (penalty-1)% to logits < 0 and substract (penalty-1)% if logits > 0\n",
    "\n",
    "            if penalty != 1:\n",
    "              for i in range(len(next_token_logits)):\n",
    "                if i in text_generated[0]:\n",
    "                  if next_token_logits[0][i]>=0:\n",
    "                    next_token_logits[0][i] *= (1-penalty)  \n",
    "                  else:\n",
    "                    next_token_logits[0][i] *= (1+penalty)\n",
    " \n",
    "            \n",
    "            cum_sum_prob = 0\n",
    "            chosen_logits_indices = []\n",
    "            sorted_softmax_values, indices = torch.sort(torch.nn.functional.softmax(next_token_logits), descending=True)\n",
    "            \n",
    "            if temperature != 0:\n",
    "                sorted_softmax_values = sorted_softmax_values/temperature\n",
    "            \n",
    "            for i in range(len(sorted_softmax_values[0])):\n",
    "              if cum_sum_prob <= p:\n",
    "                cum_sum_prob += sorted_softmax_values[0][i]\n",
    "                chosen_logits_indices.append(indices[0][i].tolist())\n",
    "              else:\n",
    "                break\n",
    "            \n",
    "            next_token_id = torch.LongTensor(random.choices(chosen_logits_indices)).unsqueeze(-1)\n",
    "            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/macbook/PycharmProjects/nlp-course/venv/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "my favourite movie is \" some life - sucking evil bitch \". \n",
      " it's a pity i don't have a gun. \n",
      " * * * \n",
      " i can't help but feel bad for leaving that day. i was only ten years old. i didn't know what to do. i had no friends. no friends. i didn't even know if i wanted to live. i had no idea if i could live without my dad. \n",
      " my dad was gone. \n",
      " my mother had died in a car accident. \n",
      " i didn't know if i wanted to live. \n",
      " my dad wasn't a great dad. he was a jerk. \n",
      " he wasn't a good man. \n",
      " i 'd had a lot of time to think about what my life would be like if i hadn't gone to college. \n",
      " i 'd been living with my mom for a year. \n",
      " i was so lonely. \n",
      " and i had a big brother. \n",
      " he was my best friend. \n",
      " i didn't want to live without him\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gen_topP_penalized_w_temp(text, 200, 0.4, 5, 1.1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}