{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"openai-gpt\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def generate_text(input_text: str, tokens_to_generate: int):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "text = \"My favourite movie is\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "text_encoded = torch.tensor([tokenizer.encode(text)], dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(text_encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "my favourite movie is the one where the guy is in love with the girl. \" \n",
      " \" i don't think that's a good idea. \" \n",
      " \" why not? \" \n",
      " \" because it's not a good idea. \" \n",
      " \" why not? \" \n",
      " \" because it's not a good idea. \" \n",
      " \" why not? \" \n",
      " \" because it's not a good idea. \" \n",
      " \" why not? \" \n",
      " \" because it's not a good idea. \" \n",
      " \" why not? \"\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(generate_text(text, 100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "def gen_topK_text(input_text: str, tokens_to_generate: int, k: int):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            \n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "            next_token_id = torch.LongTensor(random.choices(next_token_logits.topk(k, dim=-1).indices.tolist()[0], torch.nn.functional.softmax(next_token_logits).topk(k, dim=-1).values.tolist()[0])).unsqueeze(-1)            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/macbook/PycharmProjects/nlp-course/venv/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "my favourite movie is the one about that time of year where they were all together and all the boys got into a game of hide and seek. the boys were all in the front seat of the car with one boy in the back while the other boy sat on the seat beside them. they didn't have to worry about being caught and that is why they didn't want to be seen in public together. \n",
      " they got out the car and walked into the park. there were a few other boys playing games and it looked\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gen_topK_text(text, 100, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/macbook/PycharmProjects/nlp-course/venv/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "my favourite movie is the one where the guy is a vampire. he has a vampire in his arms and is about to bite her. \" \n",
      " \" that's a good movie! \" i said. \" i love that one. \" \n",
      " \" i know, \" she said. \" but it's a lot of fun to watch. \" \n",
      " we sat on the floor, watching the movie. \n",
      " \" you're so cute when you're nervous, \" she said. \" i love that. \" \n",
      " \" i love\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gen_topK_text(text, 100, 3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "def gen_topK_text_penalized(input_text: str, tokens_to_generate: int, k: int, penalty: float):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            \n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "            \n",
    "            #applying penalty (>1 to cut repetition) to logits that are already in text_generated\n",
    "            #we add (penalty-1)% to logits < 0 and substract (penalty-1)% if logits > 0\n",
    "\n",
    "            if penalty != 1:\n",
    "              for i in range(len(next_token_logits)):\n",
    "                if i in text_generated[0]:\n",
    "                  if next_token_logits[0][i]>=0:\n",
    "                    next_token_logits[0][i] *= (2-penalty)  \n",
    "                  else:\n",
    "                    next_token_logits[0][i] *= penalty\n",
    "\n",
    "            next_token_id = torch.LongTensor(random.choices(next_token_logits.topk(k, dim=-1).indices.tolist()[0], torch.nn.functional.softmax(next_token_logits).topk(k, dim=-1).values.tolist()[0])).unsqueeze(-1)   \n",
    "            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/macbook/PycharmProjects/nlp-course/venv/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "my favourite movie is the one where you're a princess and you have to be rescued from the castle. i'm not going to let you go, \" i said, pulling her into my arms. \n",
      " \" i'm not leaving you. \" she looked up at me, her eyes full of determination. \n",
      " \" i'm not going anywhere, \" i said. \n",
      " \" then let me go. \" \n",
      " \" i'm not letting you go. \" \n",
      " \" then let me go, \" she said. \" please, i need to go. \" \n",
      " \" i can't let you go, \" i said. \n",
      " \" then i 'll go. \" she pulled away from me and looked at me. \n",
      " \" i can't let you go. \" \n",
      " \" then let me go. \" \n",
      " i looked down at the floor, feeling the tears coming. \" i can't. \" \n",
      " \" then let me go. \" \n",
      " i looked up at the ceiling, feeling the tears coming. \" i can't\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gen_topK_text_penalized(text, 200, 2, 10))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "def gen_topP_text_penalized(input_text: str, tokens_to_generate: int, p: float, penalty: float):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            \n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "            \n",
    "            #applying penalty (>1 to cut repetition) to logits that are already in text_generated\n",
    "            #we add (penalty-1)% to logits < 0 and substract (penalty-1)% if logits > 0\n",
    "\n",
    "            if penalty != 1:\n",
    "              for i in range(len(next_token_logits)):\n",
    "                if i in text_generated[0]:\n",
    "                  if next_token_logits[0][i]>=0:\n",
    "                    next_token_logits[0][i] *= (2-penalty)  \n",
    "                  else:\n",
    "                    next_token_logits[0][i] *= penalty\n",
    " \n",
    "            \n",
    "            cum_sum_prob = 0\n",
    "            chosen_logits_indices = []\n",
    "            sorted_softmax_values, indices = torch.sort(torch.nn.functional.softmax(next_token_logits), descending=True)\n",
    "            \n",
    "            for i in range(len(sorted_softmax_values[0])):\n",
    "              if cum_sum_prob <= p:\n",
    "                cum_sum_prob += sorted_softmax_values[0][i]\n",
    "                chosen_logits_indices.append(indices[0][i].tolist())\n",
    "              else:\n",
    "                break\n",
    "            \n",
    "            next_token_id = torch.LongTensor(random.choices(chosen_logits_indices)).unsqueeze(-1)\n",
    "            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/macbook/PycharmProjects/nlp-course/venv/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "my favourite movie is dirty and beautiful, \" jack replied. \" that one is really beautiful. \" \n",
      " i frowned at the sad sight. i wanted to hold him close, kiss him and kiss him, but he would be afraid if i tried. so i leaned over and hugged him, taking a small piece of him into my heart and wrapping my fingers around his chin, trying to stop the tears that wanted to escape. \n",
      " \" why are you crying? \" he asked. i wiped the tears away with the back of my hand. \n",
      " \" because it was my favourite movie ever. it was my mom's favourite too, \" i said. \n",
      " \" so? \" \n",
      " \" so it was also the last time she watched it. \" \n",
      " jack was silent for a minute. his eyes filled with emotion, as he gazed at me, my beautiful beautiful face with the crooked nose, which looked perfect in the late morning light. he then took my hand and kissed it, but his touch was as awkward as ever\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gen_topP_text_penalized(text, 200, 0.7, 5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "def gen_topP_penalized_w_temp(input_text: str, tokens_to_generate: int, p: float, penalty: float, temperature: int =1):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            \n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "            \n",
    "            #applying penalty (>1 to cut repetition) to logits that are already in text_generated\n",
    "            #we add (penalty-1)% to logits < 0 and substract (penalty-1)% if logits > 0\n",
    "\n",
    "            if penalty != 1:\n",
    "              for i in range(len(next_token_logits)):\n",
    "                if i in text_generated[0]:\n",
    "                  if next_token_logits[0][i]>=0:\n",
    "                    next_token_logits[0][i] *= (2-penalty)  \n",
    "                  else:\n",
    "                    next_token_logits[0][i] *= penalty\n",
    " \n",
    "            \n",
    "            cum_sum_prob = 0\n",
    "            chosen_logits_indices = []\n",
    "            sorted_softmax_values, indices = torch.sort(torch.nn.functional.softmax(next_token_logits), descending=True)\n",
    "            \n",
    "            if temperature != 0:\n",
    "                sorted_softmax_values = sorted_softmax_values/temperature\n",
    "            \n",
    "            for i in range(len(sorted_softmax_values[0])):\n",
    "              if cum_sum_prob <= p:\n",
    "                cum_sum_prob += sorted_softmax_values[0][i]\n",
    "                chosen_logits_indices.append(indices[0][i].tolist())\n",
    "              else:\n",
    "                break\n",
    "            \n",
    "            next_token_id = torch.LongTensor(random.choices(chosen_logits_indices)).unsqueeze(-1)\n",
    "            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "    result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/macbook/PycharmProjects/nlp-course/venv/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "my favourite movie is playing. \" \n",
      " \" that's my favorite movie too. \" \n",
      " \" i know. \" he smiled. \" it's a love story. \" \n",
      " \" what do you mean? \" \n",
      " \" i mean, it's about a girl who gets a bad case of heartburn. \" \n",
      " \" what kind of girl? \" \n",
      " \" i don't know. i think she's a big girl. \" \n",
      " \" oh. \" i nodded. \" well, i'm sure she 'll be fine. \" \n",
      " \" she's not the only one. \" he laughed. \" i'm sure there are other girls out there who would love to have a girlfriend like her. \" \n",
      " \" well, i don't think she 'd want to have a boyfriend like me. \" \n",
      " \" why not? \" \n",
      " \" i don't know. \" i shrugged. \" i just don't think she 'd want to have a boyfriend like me. \" \n",
      " \" well, she's a pretty girl. \"\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(gen_topP_penalized_w_temp(text, 200, 0.4, 5, 1.1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}